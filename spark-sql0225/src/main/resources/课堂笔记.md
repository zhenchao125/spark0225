spark scala 是函数式语言

# 自定义函数

1. 自定义函数
    `spark.udf.register("myToUpper", (s: String) => s.toUpperCase())`

2. 自定义聚合函数
    `sum, avg, count ....`
    
    
# 数据源

## 1. 读
    
### 通用的读

```scala
// 默认是加载 parquet 格式
val df = spark.read.load("路径")

// 向加载 json
val df = spark.read.format("json").load(...)

```    

### 专用的读

```scala
// 专门用来读json
spark.read.json(...)
```


## 2. 写

### 通用的写
    
```scala
val df = ...

// 默认写的是parquet
df.write.save("...")

// 保存json

df.write.format("json").save(...)


```

### 专用的写

```scala
df.write.json(...)
```
    
### saveMOde
    默认: error
    overwrite
    append
    ignore\
    
# 支持Hive


## 内置Hive

直接使用


## 外置Hive

hiveserver2
    服务器  thrift 服务器
    
    可以使用客户端通过jdbc连接到hiveserver2
    
beeline
    jdbc客户端  
    可以是任何其他的客户端


## 注意

1. 创建数据仓库的时候, 尽量不要使用 spark, 默认使用spark创建的数据库的仓库在本地仓库
最后去原来的hive的地方先去创建好对应的 database

2. 如果想在spark'中创建数据库, 则需要手动指定仓库的地址

3. 默认情况, spark-shell 支持hive, 代码中默认是不支持外置hive







